![image](https://github.com/mobinanoorany/computer_vision/assets/44108288/d9cec9db-ade1-4c4a-bfac-cebe06000cba)

1: "Double Articulation in AI and Machine Learning"
Purpose:
Introduce first-year computer science students to "double articulation," exploring how data is structured at two levels for effective AI tasks.

Articulations:

High-Level Data Structuring:
•	The first articulation involves structuring data at a higher level of abstraction.
•	In AI and machine learning, this level of data structuring often includes preprocessing and feature engineering. It's about preparing the raw data in a way that makes it understandable and usable for machine learning models.

Task:

    Prepares data for machine learning.
    Includes preprocessing and feature engineering.
    Examples: Text tokenization, stemming, bag-of-words for text; resizing, normalization for images.

Low-Level Data Structuring:
•	The second articulation involves structuring data at a lower level of abstraction.
•	This level of data structuring represents data in a way that is closer to how machine learning algorithms actually process it. It often involves numerical representations, matrices, tensors, or even binary data.

Task:

    Represents data for machine processing.
    Involves numerical representations, matrices, tensors, or binary data.
    Examples: One-hot encoding, word embeddings for NLP; and pixel values as numerical arrays in images.

Connection:

    Facilitates data transition from human-readable to machine-readable forms.
    Enables efficient use of data in machine learning tasks.
    ![image](https://github.com/mobinanoorany/computer_vision/assets/44108288/7173428c-6e3e-4e32-ab70-4a2f53f354a2)

    
2: "Maps and Models in Computer Science"
Purpose:
Introduce first-year computer science students to "maps" and "models," building a foundational understanding of data structures and machine learning.

Part 1: Maps

    Explore data types: images or numerical.
    Learn data structure for machine learning.
    Perform Python operations on dictionaries.
    Write a Python function for mapping numerical data.
    Understand pixel-wise maps for image representation.

Part 2: Models (Machine Learning):

    Choose an image classification problem (e.g., Flower Recognition or MNIST).
    Use a machine learning library (e.g., TensorFlow or PyTorch).
    Build, train, and describe a CNN model.
    Apply the model to classify images.

 3: "Entropy and Negentropy in Information Theory and AI"
Purpose:
Introduce students to entropy and negentropy, showcasing their applications in AI.
Task:
    Apply entropy and negentropy to feature selection and data preprocessing in machine learning.
    Demonstrate in a specific AI application (e.g., spam email classification, sentiment analysis).

Entropy:
Entropy is a measure of the uncertainty or disorder in a dataset. In information theory, it quantifies the amount of information contained in a set of data. High entropy indicates high uncertainty, randomness, or disorder, while low entropy signifies more predictable or structured data.

Task:

    Measure of uncertainty or disorder.
    Used in decision trees and classification.
    Application in natural language processing for text classification.

Negentropy:
Negentropy is essentially the opposite of entropy. It measures the presence of order or structured information in a dataset. It quantifies how far a dataset departs from perfect randomness or disorder. High negentropy suggests a dataset contains structured information or patterns.

Task:

    Measures structured information.
    Applied in feature selection for pattern recognition.
    Used in image analysis for identifying regular patterns.
